---
title: "Data Science Capstone Milestone Report"
author: "Codrin Kruijne"
date: "12/05/2018"
output:
  html_document:
    df_print: paged
---

## Capstone project

The goal of the capstone proejct is to create a Shiny Web app that provides predicive text suggestion for the possible following word while typing.

###  Technologies used
My aim was to get familiar with the [TidyText package](https://www.tidytextmining.com/index.html) for text mining that works well with other tidy packages for R. 

```{r message=FALSE, warning=FALSE}

require(tm)
require(tidyr)
require(stringr)
require(ggplot2)
require(gridExtra)
require(microbenchmark)

```

## Training data

Three files were provided with a selection of tweets, news items and blog entries that were obtained by web crawling.

### Obtaining data

```{r cache=TRUE}

require(readr)

twitter_txt <- read_lines("en_US.twitter.txt")
names(twitter_txt) <- paste0("tweet", 1:length(twitter_txt))
twitter_df <- data_frame("text" = twitter_txt)

news_txt <- read_lines("en_US.news.txt")
names(news_txt) <- paste0("item", 1:length(news_txt))
news_df <- data_frame("text" = news_txt)

blogs_txt <- read_lines("en_US.blogs.txt")
names(blogs_txt) <- paste0("post", 1:length(blogs_txt))
blogs_df <- data_frame("text" = blogs_txt)


format(object.size(twitter_df), units = "auto")
format(object.size(news_df), units = "auto")
format(object.size(blogs_df), units = "auto")

```

### Extracting samples

Given the rather large files I will take 10 random entries from each source to explore.

```{r message=FALSE, warning=FALSE, cache=TRUE}

require(dplyr)

dim(twitter_df)
dim(news_df)
dim(blogs_df)

twitter_sample <- sample_frac(twitter_df, 0.01)
news_sample <- sample_frac(news_df, 0.01)
blogs_sample <- sample_frac(blogs_df, 0.01)

dim(twitter_sample)
dim(news_sample)
dim(blogs_sample)

```

## Creating a corpus

```{r cache=TRUE}
require(quanteda)

## Turning samples into collection data frame

collection_sample <- bind_rows("twitter" = twitter_sample, "news" = news_sample, "blogs" = blogs_sample, .id = "source")
colnames(collection_sample) <- c("source", "text")
collection_sample$source <- factor(collection_sample$source)
dim(collection_sample)

# Set training sample

training_data <- collection_sample

# Creating a corpus

training_corpus <- corpus(training_data)

```

### Preprocessing

Removing:
- Profanity filtering
- Twitter text
- links etc.
- numbers
- smileys
- abbreviations

Changing
- word combinations to full: can't -> cannot

```{r cache=TRUE}

# QUANTEDA Tokenizing and preprocessing corpus

system.time(collection_dfm <- dfm(training_corpus,
                                  remove_numbers = TRUE,
                                  remove_punct = TRUE,
                                  remove_symbols = TRUE,
                                  remove_separators = TRUE,
                                  remove_twitter = TRUE,
                                  remove_hyphens = TRUE,
                                  remove_url = TRUE))

clean_collection_df <- convert(collection_dfm, to = "data.frame")

```


## Language modeling

For ngrams we calculate Maximum Likelihood Estimators by counting ngram frequency of folling word, divided by count of that word.

```{r cache=TRUE}

require(tidytext)

# training_data <- clean_collection_df

# TIDY Extract ngrams

print("Time to extract bigrams:")
system.time(bigrams <- training_data %>% unnest_tokens(bigram, text, token = "ngrams", n = 2))
str(bigrams)
print("Time to extract trigrams:")
system.time(trigrams <- training_data %>% unnest_tokens(trigram, text, token = "ngrams", n = 3))
str(trigrams)
print("Time to extract fourgrams:")
system.time(fourgrams <- training_data %>% unnest_tokens(fourgram, text, token = "ngrams", n = 4))
str(fourgrams)

# TIDY Separate bigram words into columns for counting

print("Time to split bigrams:")
system.time(split_bigrams <- bigrams %>% separate(bigram, into = c("base", "prediction"), sep = " "))
str(split_bigrams)
print("Time to split trigrams:")
system.time(split_trigrams <- trigrams %>% separate(trigram, into = c("base1", "base2", "prediction"), sep = " ") %>% unite(base1, base2, col = "base", sep = " "))
str(split_trigrams)
print("Time to split fourgrams:")
system.time(split_fourgrams <- fourgrams %>% separate(fourgram, into = c("base1", "base2","base3", "prediction"), sep = " ") %>% unite(base1, base2, base3, col = "base", sep = " "))
str(split_fourgrams)


# Counting following words

require(data.table)

mle_calculator <- function(split_ngram) {
  
  split_ngram <- data.table(split_ngram)
  split_ngram[, .(prediction, norm = .N), by = .(base) # count base frequency to normalise
              ][, .(norm, frequency = .N), by = .(base, prediction) # count ngram frequency
                ][, .(base, prediction, norm, frequency, mle = frequency / norm)] # calculate MLE
  
}

print("Time to calculate bigram MLEs:")
system.time(bigram_mle <- mle_calculator(split_bigrams))
print("Time to calculate trigram MLEs:")
system.time(trigram_mle <- mle_calculator(split_trigrams))
print("Time to calculate fourgram MLEs:")
system.time(fourgram_mle <- mle_calculator(split_fourgrams))

# Smoothing: add-one of count equals zero

# n-grams that are aggregated into three columns, a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

```

## Looking up the most probable next word given an input string

```{r cache=TRUE}

# Now lets create a lookup table from the raw bigram probabilities

# Lookup data.table per ngram consisting of three columns: a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

ngram_probs <- function(ngram_mle) {

  unique(ngram_mle[, .(mle), by = .(base, prediction) # select relevant, unique rows
             ][order(base, -mle)]) # order by mle
  
}

print("Time to create bigram lookup table:")
system.time(bigram_lookup <- ngram_probs(bigram_mle))
print("Time to create trigram lookup table:")
system.time(trigram_lookup <- ngram_probs(trigram_mle))
print("Time to create fourgram lookup table:")
system.time(fourgram_lookup <- ngram_probs(fourgram_mle))

# Lets save the lookup table as an rds file to send with Shiny App

ngram_lookup <- rbindlist(list(bigram_lookup, trigram_lookup, fourgram_lookup))
saveRDS(ngram_lookup, file = "WordPredictor/data/server_lookup.rds")

# A lookup function: given a string, look up the most probable next word

predictWord <- function(string, ngram_lookup) {
  
  # What ngam is the string?

  # Lookup ngram in data.table organised by ngram probability

  # If not found _back off_ to smaller ngram prediction
  
  ngram_lookup[base == string][1]$prediction
  
}

```


## Ideas for improvement

- Add keys to data.tables
- Add background language model:
<https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html>
<https://storage.googleapis.com/books/ngrams/books/datasetsv2.html>
- More sophisticated smoothing