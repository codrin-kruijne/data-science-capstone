---
title: "Data Science Capstone Milestone Report"
author: "Codrin Kruijne"
date: "12/05/2018"
output:
  html_document:
    df_print: paged
---

## Capstone project

The goal of the capstone proejct is to create a Shiny Web app that provides predicive text suggestion for the possible following word while typing.

###  Technologies used
My aim was to get familiar with the [TidyText package](https://www.tidytextmining.com/index.html) for text mining that works well with other tidy packages for R. 

```{r message=FALSE, warning=FALSE}

require(tidyr)
require(dplyr)
require(stringr)

require(ggplot2)
require(gridExtra)

require(tm)

require(microbenchmark)
require(parallel)
```

## Training data

Three files were provided with a selection of tweets, news items and blog entries that were obtained by web crawling.

### Obtaining data

```{r cache=TRUE}

require(readr)

twitter_txt <- read_lines("en_US.twitter.txt")
names(twitter_txt) <- paste0("tweet", 1:length(twitter_txt))
twitter_df <- data_frame("text" = twitter_txt)

news_txt <- read_lines("en_US.news.txt")
names(news_txt) <- paste0("item", 1:length(news_txt))
news_df <- data_frame("text" = news_txt)

blogs_txt <- read_lines("en_US.blogs.txt")
names(blogs_txt) <- paste0("post", 1:length(blogs_txt))
blogs_df <- data_frame("text" = blogs_txt)


format(object.size(twitter_df), units = "auto")
format(object.size(news_df), units = "auto")
format(object.size(blogs_df), units = "auto")

```

### Extracting samples

Given the rather large files I will take 10 random entries from each source to explore.

```{r message=FALSE, warning=FALSE, cache=TRUE}

dim(twitter_df)
dim(news_df)
dim(blogs_df)

sample_size <- 0.1
twitter_sample <- sample_frac(twitter_df, sample_size)
news_sample <- sample_frac(news_df, sample_size)
blogs_sample <- sample_frac(blogs_df, sample_size)

dim(twitter_sample)
dim(news_sample)
dim(blogs_sample)

```

## Creating a corpus

```{r cache=TRUE}
require(quanteda)

## Turning samples into collection data frame

collection_sample <- bind_rows("twitter" = twitter_sample, "news" = news_sample, "blogs" = blogs_sample, .id = "source")
colnames(collection_sample) <- c("source", "text")
collection_sample$source <- factor(collection_sample$source)
dim(collection_sample)

# Set training sample

training_data <- collection_sample

# Creating corpi

print("Time to create corpus: Twitter")
system.time(twitter_corpus <- corpus(twitter_sample))
print("Time to create corpus: News")
system.time(news_corpus <- corpus(news_sample))
print("Time to create corpus: Blogs")
system.time(blogs_corpus <- corpus(blogs_sample))
print("Time to create corpus: Collection")
system.time(training_corpus <- corpus(collection_sample))

```

### Preprocessing

Removing:
- Profanity filtering
- Twitter text
- links etc.
- numbers
- smileys
- abbreviations

Changing
- word combinations to full: can't -> cannot

```{r cache=TRUE}

# QUANTEDA Function for preprocessing corpus and tokenizing ngrams

require(data.table)

create_ngrams_dt <- function(q_corpus_n) {

  corpus_dfm <- quanteda::dfm(q_corpus_n[[1]],
                    tolower = TRUE,
                    remove_numbers = TRUE,
                    remove_punct = TRUE,
                    remove_symbols = TRUE,
                    remove_separators = TRUE,
                    remove_twitter = TRUE,
                    remove_hyphens = TRUE,
                    remove_url = TRUE,
                    ngrams = q_corpus_n[[2]],
                    concatenator = " ",
                    verbose = TRUE)
  
  dt <- data.table::data.table(ngrams = corpus_dfm@Dimnames$features)
  
}

# Parallel preprocessing and all ngam creation for all corpi

cl <- makeCluster(10)

parallel_list <- list(list(twitter_corpus, 2),
                      list(twitter_corpus, 3),
                      list(twitter_corpus, 4),
                      list(news_corpus, 2),
                      list(news_corpus, 3),
                      list(news_corpus, 4),
                      list(blogs_corpus, 2),
                      list(blogs_corpus, 3),
                      list(blogs_corpus, 4))
print("Time of preprocessing and creating all bigrams: Collection PARALLEL")
system.time(collection_bigrams_dt <- parLapply(cl, parallel_list, create_ngrams_dt))

stopCluster(cl)

# QUANTEDA Preprocessing and ngram creation from Twitter: DT

print("Time of preprocessing creating bigrams: Twitter")
system.time(twitter_bigrams <- create_ngrams_dt(twitter_corpus, 2))
print("Time of preprocessing creating trigrams: Twitter")
system.time(twitter_trigrams <- create_ngrams_dt(twitter_corpus, 3))
print("Time of preprocessing creating ourgrams: Twitter")
system.time(twitter_fourgrams <- create_ngrams_dt(twitter_corpus, 4))

# QUANTEDA Preprocessing and ngram creation from News: DT

print("Time of preprocessing creating bigrams: News")
system.time(news_bigrams <- create_ngrams_dt(news_corpus, 2))
print("Time of preprocessing creating trigrams: News")
system.time(news_trigrams <- create_ngrams_dt(news_corpus, 3))
print("Time of preprocessing creating fourgrams: News")
system.time(news_fourgrams <- create_ngrams_dt(news_corpus, 4))

# QUANTEDA Preprocessing and ngram creation from Blogs: DT

print("Time of preprocessing creating bigrams: Blogs")
system.time(blogs_bigrams <- create_ngrams_dt(blogs_corpus, 2))
print("Time of preprocessing creating trigrams: Blogs")
system.time(blogs_trigrams <- create_ngrams_dt(blogs_corpus, 3))
print("Time of preprocessing creating fourgrams: Blogs")
system.time(blogs_fourgrams <- create_ngrams_dt(blogs_corpus, 4))

# QUANTEDA Preprocessing and ngram creation from Collection: DT

print("Time of preprocessing creating bigrams: Collection")
system.time(collection_bigrams <- create_ngrams_dt(training_corpus, 2))
print("Time of preprocessing creating trigrams: Collection")
system.time(collection_trigrams <- create_ngrams_dt(training_corpus, 3))
print("Time of preprocessing creating fourgrams: Collection")
system.time(collection_fourgrams <- create_ngrams_dt(training_corpus, 4))

# QUANTEDA All at once corpus

all_ngrams_dt <- function(q_corpus) {

  corpus_dfm <- quanteda::dfm(q_corpus,
                    tolower = TRUE,
                    remove_numbers = TRUE,
                    remove_punct = TRUE,
                    remove_symbols = TRUE,
                    remove_separators = TRUE,
                    remove_twitter = TRUE,
                    remove_hyphens = TRUE,
                    remove_url = TRUE,
                    ngrams = 2:4,
                    concatenator = " ",
                    verbose = TRUE)
  
  dt <- data.table::data.table(ngrams = corpus_dfm@Dimnames$features)
  
}

print("Time of preprocessing and creating all ngrams: Collection")
system.time(collection_ngrams <- all_ngrams_dt(training_corpus))

# Trying parallel processing 2

cl <- makeCluster(3)

print("Time of preprocessing and creating all ngrams: Collection PARALLEL")
system.time(collection_ngrams_dt <- parLapply(cl, list(twitter_corpus, news_corpus, blogs_corpus), all_ngrams_dt))

stopCluster(cl)


# Merging ngrams

twitter_dt <- rbindlist(list(twitter_bigrams, twitter_trigrams, twitter_fourgrams))
news_dt <- rbindlist(list(news_bigrams, news_trigrams, news_fourgrams))
blogs_dt <- rbindlist(list(blogs_bigrams, blogs_trigrams, blogs_fourgrams))


bigrams_dt <- rbindlist(list(twitter_bigrams, news_bigrams, blogs_bigrams))
trigrams_dt <- rbindlist(list(twitter_trigrams, news_trigrams, blogs_trigrams))
fourgrams_dt <- rbindlist(list(twitter_trigrams, news_trigrams, blogs_trigrams))
  
ngrams_dt <- rbindlist(list(bigrams_dt, trigrams_dt, fourgrams_dt))

```


## Language modeling

For ngrams we calculate Maximum Likelihood Estimators by counting ngram frequency of folling word, divided by count of that word.

```{r cache=TRUE}

require(tidytext)

# training_data <- clean_collection_df

# TIDY Extract ngrams

print("Time to extract bigrams:")
system.time(bigrams <- training_data %>% unnest_tokens(bigram, text, token = "ngrams", n = 2))
str(bigrams)
print("Time to extract trigrams:")
system.time(trigrams <- training_data %>% unnest_tokens(trigram, text, token = "ngrams", n = 3))
str(trigrams)
print("Time to extract fourgrams:")
system.time(fourgrams <- training_data %>% unnest_tokens(fourgram, text, token = "ngrams", n = 4))
str(fourgrams)

# QUANTEDA Separate bigram words into columns for counting

print("Time to split bigrams DT:")
system.time(split_bigrams_dt <- bigrams_dt %>% separate(ngrams, into = c("base_bi", "prediction"), sep = " "))
str(split_bigrams_dt)

print("Time to split trigrams DT:")
system.time(split_trigrams_dt <- trigrams_dt %>% separate(ngrams, into = c("base_tri",  "base_bi", "prediction"), sep = " "))
str(split_bigrams_dt)

print("Time to split fourgrams DT:")
system.time(split_fourgrams_dt <- fourgrams_dt %>% separate(ngrams, into = c("base_four", "base_tri", "base_bi", "prediction"), sep = " "))
str(split_fourgrams_dt)

# QUANTEDA All at once

print("Time to split Twitter DT:") 
system.time(split_twitter_dt <- twitter_dt %>% separate(ngrams, into = c("base_four", "base_tri","base_bi", "prediction"), sep = " ", fill = "left"))
str(split_twitter_dt)

print("Time to split News DT:") 
system.time(split_news_dt <- news_dt %>% separate(ngrams, into = c("base_four", "base_tri","base_bi", "prediction"), sep = " ", fill = "left"))
str(split_news_dt)

print("Time to split Blogs DT:") 
system.time(split_blogs_dt <- blogs_dt %>% separate(ngrams, into = c("base_four", "base_tri","base_bi", "prediction"), sep = " ", fill = "left"))
str(split_blogs_dt)

print("Time to split Collection DT:") 
system.time(split_ngrams_dt <- ngrams_dt %>% separate(ngrams, into = c("base_four", "base_tri","base_bi", "prediction"), sep = " ", fill = "left"))
str(split_ngrams_dt)
format(object.size(split_ngrams_dt), units = "auto")

# TIDY Separate bigram words into columns for counting

print("Time to split bigrams:")
system.time(split_bigrams <- bigrams %>% separate(bigram, into = c("base", "prediction"), sep = " "))
str(split_bigrams)

print("Time to split trigrams:")
system.time(split_trigrams <- trigrams %>% separate(trigram, into = c("base1", "base2", "prediction"), sep = " ") %>% unite(base1, base2, col = "base", sep = " "))
str(split_trigrams)

print("Time to split fourgrams:")
system.time(split_fourgrams <- fourgrams %>% separate(fourgram, into = c("base1", "base2","base3", "prediction"), sep = " ") %>% unite(base1, base2, base3, col = "base", sep = " "))
str(split_fourgrams)

```

```{r cache=TRUE}

# Counting Maximum Likelihood Estimators

mle_calculator <- function(split_ngram) {
  
  split_ngram <- data.table(split_ngram)
  split_ngram[, .(prediction, norm = .N), by = .(base) # count base frequency to normalise
              ][, .(norm, frequency = .N), by = .(base, prediction) # count ngram frequency
                ][, .(base, prediction, norm, frequency, mle = frequency / norm)] # calculate MLE
  
}

print("Time to calculate bigram MLEs:")
system.time(bigram_mle <- mle_calculator(split_bigrams))
print("Time to calculate trigram MLEs:")
system.time(trigram_mle <- mle_calculator(split_trigrams))
print("Time to calculate fourgram MLEs:")
system.time(fourgram_mle <- mle_calculator(split_fourgrams))

# Smoothing: add-one of count equals zero

# n-grams that are aggregated into three columns, a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

```

## Looking up the most probable next word given an input string

```{r cache=TRUE}

# Now lets create a lookup table from the raw bigram probabilities

# Lookup data.table per ngram consisting of three columns: a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

ngram_probs <- function(ngram_mle) {

  unique(ngram_mle[, .(mle), by = .(base, prediction) # select relevant, unique rows
             ][order(base, -mle)]) # order by mle
  
}

print("Time to create bigram lookup table:")
system.time(bigram_lookup <- ngram_probs(bigram_mle))
print("Time to create trigram lookup table:")
system.time(trigram_lookup <- ngram_probs(trigram_mle))
print("Time to create fourgram lookup table:")
system.time(fourgram_lookup <- ngram_probs(fourgram_mle))

# Lets save the lookup table as an rds file to send with Shiny App

ngram_lookup <- rbindlist(list(bigram_lookup, trigram_lookup, fourgram_lookup))
saveRDS(ngram_lookup, file = "WordPredictor/data/server_lookup.rds")

# A lookup function: given a string, look up the most probable next word

predictWord <- function(string, ngram_lookup) {
  
  # Lookup ngram in data.table organised by ngram probability
  
  ngram_lookup[base == string][1]$prediction
  
}

```


## Ideas for improvement

- Add keys to data.tables
- Add background language model:
<https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html>
<https://storage.googleapis.com/books/ngrams/books/datasetsv2.html>
- More sophisticated smoothing