---
title: "Data Science Capstone Report"
author: "Codrin Kruijne"
date: "10/06/2018"
output:
  html_document:
    df_print: paged
---

## Capstone project

The goal of the capstone proejct is to create a Shiny Web app that provides predicive text suggestion for the possible following word while typing.

###  Technologies used
My aim was to get familiar with the [TidyText package](https://www.tidytextmining.com/index.html) for text mining that works well with other tidy packages for R. 

```{r message=FALSE, warning=FALSE}

require(tidyr)
require(dplyr)
require(stringr)

require(ggplot2)
require(gridExtra)

require(tm)

require(microbenchmark)
require(parallel)
```

## Training data

Three files were provided with a selection of tweets, news items and blog entries that were obtained by web crawling.

### Obtaining data

```{r cache=TRUE}

require(readr)

twitter_txt <- read_lines("en_US.twitter.txt")
news_txt <- read_lines("en_US.news.txt")
blogs_txt <- read_lines("en_US.blogs.txt")

format(object.size(twitter_txt), units = "auto")
format(object.size(news_txt), units = "auto")
format(object.size(blogs_txt), units = "auto")

```

### Extracting samples

Given the rather large files I will take 10 random entries from each source to explore.

```{r message=FALSE, warning=FALSE, cache=TRUE}

print("Data rows: Twitter, News, Blogs")
length(twitter_txt)
length(news_txt)
length(blogs_txt)

sample_frac <- 0.5
twitter_sample <- sample(twitter_txt, sample_frac * length(twitter_txt))
news_sample <- sample(news_txt, sample_frac * length(news_txt))
blogs_sample <- sample(blogs_txt, sample_frac * length(blogs_txt))

print("Sample rows: Twitter, News, Blogs")
length(twitter_sample)
length(news_sample)
length(blogs_sample)

```

## Creating a corpus

```{r cache=TRUE}

require(quanteda)

# Creating corpi PARALLEL

cl1 <- makeCluster(3)

print("Time to create corpi PARALLEL")
system.time(corpus_list <- parLapply(cl1, list(twitter_sample, news_sample, blogs_sample), corpus))

stopCluster(cl1)

# Extract results

twitter_corpus <- corpus_list[[1]]
news_corpus <- corpus_list[[2]]
blogs_corpus <- corpus_list[[3]]

```

### Preprocessing

Removing:
- Profanity filtering
- smileys
- abbreviations

Changing:
- word combinations to full: can't -> cannot

Stemming?

```{r cache=TRUE}

# QUANTEDA Function for preprocessing corpus and tokenizing ngrams

require(data.table)

create_ngrams_dt <- function(q_corpus_n) {

  corpus_dfm <- quanteda::dfm(q_corpus_n[[1]],
                              tolower = TRUE,
                              remove_numbers = TRUE,
                              remove_punct = TRUE,
                              remove_symbols = TRUE,
                              remove_separators = TRUE,
                              remove_twitter = TRUE,
                              remove_hyphens = TRUE,
                              remove_url = TRUE,
                              ngrams = q_corpus_n[[2]],
                              concatenator = " ",
                              verbose = TRUE)
  
  dt <- data.table::data.table(ngrams = corpus_dfm@Dimnames$features)
  
}

# Parallel preprocessing and all ngam creation for all corpi

cl2 <- makeCluster(12)

parallel_list <- list(list(twitter_corpus, 2),
                      list(twitter_corpus, 3),
                      list(twitter_corpus, 4),
                      list(twitter_corpus, 5),
                      list(news_corpus, 2),
                      list(news_corpus, 3),
                      list(news_corpus, 4),
                      list(news_corpus, 5),
                      list(blogs_corpus, 2),
                      list(blogs_corpus, 3),
                      list(blogs_corpus, 4),
                      list(blogs_corpus, 5))

print("Time of preprocessing and creating all bigrams: Collection")
system.time(collection_ngrams_list <- parLapply(cl2, parallel_list, create_ngrams_dt))

stopCluster(cl2)

rm(parallel_list) # Free up memory

# Extracting parallell processing results

twitter_bi_dt <- collection_ngrams_list[[1]]
twitter_tri_dt <- collection_ngrams_list[[2]]
twitter_four_dt <- collection_ngrams_list[[3]]
twitter_five_dt <- collection_ngrams_list[[4]]

news_bi_dt <- collection_ngrams_list[[5]]
news_tri_dt <- collection_ngrams_list[[6]]
news_four_dt <- collection_ngrams_list[[7]]
news_five_dt <- collection_ngrams_list[[8]]

blogs_bi_dt <- collection_ngrams_list[[9]]
blogs_tri_dt <- collection_ngrams_list[[10]]
blogs_four_dt <- collection_ngrams_list[[11]]
blogs_five_dt <- collection_ngrams_list[[12]]

# twitter_dt <- rbindlist(collection_ngrams_list[1:4])
# news_dt <- rbindlist(collection_ngrams_list[5:8])
# blogs_dt <- rbindlist(collection_ngrams_list[9:12])

bigrams_dt <- rbindlist(collection_ngrams_list[c(1, 5, 9)])
trigrams_dt <- rbindlist(collection_ngrams_list[c(2, 6, 10)])
fourgrams_dt <- rbindlist(collection_ngrams_list[c(3, 7, 11)])
fivegrams_dt <- rbindlist(collection_ngrams_list[c(4, 8, 12)])
  
# ngrams_dt <- rbindlist(collection_ngrams_list)

# Succesfull ngram creation? Clean up!
rm(collection_ngrams_list)

```


## Language modeling

For ngrams we calculate Maximum Likelihood Estimators by counting ngram frequency of folling word, divided by count of that word.

```{r cache=TRUE}

require(tidytext)

# function for splitting and ngrams to base and prediction columns
# that takes a list of an ngram_dt and a split list

split_merge_dt <- function(ngram_split){
  
  # split to separate columns
  result_dt <- tidyr::separate(ngram_split[[1]], ngrams, into = ngram_split[[2]], sep = " ")
  
  # unite trigram and fourgram bases
  if(length(ngram_split[[2]]) == 3){
    result_dt <- tidyr::unite(result_dt, base_tri, base_bi, col = "base", sep = " ")
  }
  else if(length(ngram_split[[2]]) == 4){
    result_dt <- tidyr::unite(result_dt, base_four, base_tri, base_bi, col = "base", sep = " ")
  }
  else if(length(ngram_split[[2]]) == 5){
    result_dt <- tidyr::unite(result_dt, base_five, base_four, base_tri, base_bi, col = "base", sep = " ")
  }
  result_dt

}

# Preparing parallel processing of splitting and merging

cl3 <- makeCluster(12)

# create a list of split options for each ngram
split_list <- list(c("base", "prediction"),
                   c("base_tri",  "base_bi", "prediction"),
                   c("base_four", "base_tri", "base_bi", "prediction"),
                   c("base_five", "base_four", "base_tri", "base_bi", "prediction"))

# create a list of lists: each with a ngram dt and a split list
parallel_list2 <- list(list(twitter_bi_dt, split_list[[1]]),
                      list(twitter_tri_dt, split_list[[2]]),
                      list(twitter_four_dt, split_list[[3]]),
                      list(twitter_five_dt, split_list[[4]]),
                      list(news_bi_dt, split_list[[1]]),
                      list(news_tri_dt, split_list[[2]]),
                      list(news_four_dt, split_list[[3]]),
                      list(news_five_dt, split_list[[4]]),
                      list(blogs_bi_dt, split_list[[1]]),
                      list(blogs_tri_dt, split_list[[2]]),
                      list(blogs_four_dt, split_list[[3]]),
                      list(blogs_five_dt, split_list[[4]]))

print("Time of splitting and merging words: Collection")
system.time(collection_base_pred_list <- parLapply(cl3, parallel_list2, split_merge_dt))

stopCluster(cl3)

# Extracting parallell processing results

# twitter_split_bi_dt <- collection_base_pred_list[[1]]
# twitter_split_tri_dt <- collection_base_pred_list[[2]]
# twitter_split_four_dt <- collection_base_pred_list[[3]]
# twitter_split_five_dt <- collection_base_pred_list[[4]]
# 
# news_split_bi_dt <- collection_base_pred_list[[5]]
# news_split_tri_dt <- collection_base_pred_list[[6]]
# news_split_four_dt <- collection_base_pred_list[[7]]
# news_split_five_dt <- collection_base_pred_list[[8]]
# 
# blogs_split_bi_dt <- collection_base_pred_list[[9]]
# blogs_split_tri_dt <- collection_base_pred_list[[10]]
# blogs_split_four_dt <- collection_base_pred_list[[11]]
# blogs_split_five_dt <- collection_base_pred_list[[12]]

# twitter_split_dt <- rbindlist(collection_base_pred_list[1:4])
# news_split_dt <- rbindlist(collection_base_pred_list[5:8])
# blogs_split_dt <- rbindlist(collection_base_pred_list[9:12])

split_bigrams_dt <- rbindlist(collection_base_pred_list[c(1, 5, 9)])
split_trigrams_dt <- rbindlist(collection_base_pred_list[c(2, 6, 10)])
split_fourgrams_dt <- rbindlist(collection_base_pred_list[c(3, 7, 11)])
split_fivegrams_dt <- rbindlist(collection_base_pred_list[c(4, 8, 12)])
  
# split_ngrams_dt <- rbindlist(collection_base_pred_list)

# Succesful base-prediction splitting? Clean up!
rm(parallel_list2)
rm(collection_base_pred_list)
rm(bigrams_dt)
rm(trigrams_dt)
rm(fourgrams_dt)

```

```{r cache=TRUE}

# Counting Maximum Likelihood Estimators

mle_calculator <- function(split_ngram) {
  
  split_ngram <- data.table::data.table(split_ngram)
  split_ngram[, .(prediction, norm = .N), by = .(base) # count base frequency to normalise
              ][, .(norm, frequency = .N), by = .(base, prediction) # count ngram frequency
                ][, .(base, prediction, norm, frequency, mle = frequency / norm)] # calculate MLE
  
}

# MLE calculation PARALLEL

# cl4 <- makeCluster(4)
# 
# print("Time to calculate MLEs PARALLEL")
# system.time(mle_list <- parLapply(cl4,
#                                   list(split_bigrams_dt,
#                                        split_trigrams_dt,
#                                        split_fourgrams_dt),
#                                   mle_calculator))
# 
# stopCluster(cl4)
# 
# bigram_mle <- mle_list[[1]]
# trigram_mle <- mle_list[[2]]
# fourgram_mle <- mle_list[[3]]

# Basic MLE calculation SEEMS FASTER?!

system.time(bigram_mle <- mle_calculator(split_bigrams_dt))
system.time(trigram_mle <- mle_calculator(split_trigrams_dt))
system.time(fourgram_mle <- mle_calculator(split_fourgrams_dt))
system.time(fivegram_mle <- mle_calculator(split_fivegrams_dt))

# Succesful MLE calculation? Clean up!
# rm(mle_list)
rm(split_bigrams_dt)
rm(split_trigrams_dt)
rm(split_fourgrams_dt)
rm(split_fivegrams_dt)

# Smoothing: add-one of count equals zero

# n-grams that are aggregated into three columns, a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

```

## Looking up the most probable next word given an input string

```{r cache=TRUE}

# Now lets create a lookup table from the raw bigram probabilities

# Lookup data.table per ngram consisting of three columns: a base consisting of n-1 words in the n-gram, and a prediction that is the last word, and a count variable for the frequency of occurrence of this n-gram

ngram_probs <- function(ngram_mle) {

  dt <- data.table::data.table(ngram_mle)
  unique(dt[, .(mle), by = .(base, prediction) # select relevant, unique rows
             ][order(base, -mle)]) # order by mle
  
}

# Lookup table creation PARALLEL

# cl5 <- makeCluster(4)
# 
# print("Time to create lookup tables PARALLEL")
# system.time(lookup_list <- parLapply(cl5,
#                                      list(bigram_mle,
#                                           trigram_mle,
#                                           fourgram_mle,
#                                           fivegram_mle),
#                                      ngram_probs))
# 
# stopCluster(cl5)
# 
# 
# bigram_lookup <- lookup_list[[1]]
# trigram_lookup <- lookup_list[[2]] 
# fourgram_lookup <- lookup_list[[3]]
# fivegram_lookup <- lookup_list[[4]]

# Basic lookup table creation SEEMS FASTER?!

system.time(bigram_lookup <- ngram_probs(bigram_mle))
system.time(trigram_lookup <- ngram_probs(trigram_mle))
system.time(fourgram_lookup <- ngram_probs(fourgram_mle))
system.time(fivegram_lookup <- ngram_probs(fivegram_mle))

# Lets save the lookup table as an rds file to send with Shiny App

ngram_lookup <- rbindlist(list(bigram_lookup, trigram_lookup, fourgram_lookup, fivegram_lookup))
saveRDS(ngram_lookup, file = "WordPredictor/data/server_lookup.rds")

# Succesfull ngram lookup? Clean up!
# rm(lookup_list)
rm(bigram_lookup)
rm(trigram_lookup)
rm(fourgram_lookup)

# A lookup function: given a string, look up the most probable next word

predictWord <- function(string, ngram_lookup) {
  
  # Lookup ngram in data.table organised by ngram probability
  
  ngram_lookup[base == string][1]$prediction
  
}

```


## Ideas for improvement

- Add keys to data.tables
- Add background language model:
<https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html>
<https://storage.googleapis.com/books/ngrams/books/datasetsv2.html>
- More sophisticated smoothing