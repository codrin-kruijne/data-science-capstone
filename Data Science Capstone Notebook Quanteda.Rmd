---
title: "Data Science Capstone Notebook"
author: "Codrin Kruijne"
output:
  html_document:
    df_print: paged
---

This notebook for the Data Science pecialization Capstone course.

## Capstone project

The goal of the capstone proejct is to create a Shiny Web app that provides predicive text suggestion for the possible following word while typing.

## Week 1

## Task 0: Understanding the problem

### Familiarizing yourself with NLP and text mining

Learn about the basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.


###  Technologies used
My aim is to learn text mining using the [Quanteda package](http://docs.quanteda.io/index.html) for text management and analysis.
- [Quanteda Quick Start Guide](http://docs.quanteda.io/articles/pkgdown/quickstart.html)
- [Quanteda tutorial](https://tutorials.quanteda.io)
- [Quanteda function reference](http://docs.quanteda.io/reference/)

## Task 1: Getting and cleaning the data

```{r cache=TRUE}
require(quanteda)
require(readtext)
require(tidytext)
require(readr)
require(dplyr)
require(ggplot2)
```

### Obtaining the data

Can you download the data and load/manipulate it in R?

```{r cache=TRUE}

# Reading files

twitter_txt <- read_lines("en_US.twitter.txt")
names(twitter_txt) <- paste0("tweet", 1:length(twitter_txt))
twitter_df <- data_frame("text" = twitter_txt, id = 1:length(twitter_txt))

news_txt <- read_lines("en_US.news.txt")
names(news_txt) <- paste0("item", 1:length(news_txt))
news_df <- data_frame("text" = news_txt, id = 1:length(news_txt))

blogs_txt <- read_lines("en_US.blogs.txt")
names(blogs_txt) <- paste0("post", 1:length(blogs_txt))
blogs_df <- data_frame("text" = blogs_txt, id = 1:length(blogs_txt))

# Creating corpi

twitter_corpus <- corpus(twitter_df)
news_corpus <- corpus(news_df)
blogs_corpus <- corpus(blogs_df)

collection <- bind_rows("twitter" = twitter_df, "news" = news_df, "blogs" = blogs_df, .id = "type")
collection_corpus <- corpus(collection)

# lets take samples while we figure things out

twitter_sample <- corpus_sample(twitter_corpus, 100)
news_sample <- corpus_sample(news_corpus, 100)
blogs_sample <- corpus_sample(blogs_corpus, 100)

collection_sample <- corpus_sample(collection_corpus, 100)
```

## Tokenization and ngrams

```{r cache=TRUE}

# collection tokens
collection_tokens <- tokens(collection_sample, remove_numbers = TRUE, remove_punct = TRUE,
  remove_symbols = TRUE, remove_separators = TRUE,
  remove_twitter = TRUE, remove_hyphens = TRUE, remove_url = TRUE)

# ngrams
collection_unigrams <- collection_tokens
collection_bigrams <- tokens_ngrams(collection_tokens, n = 2, concatenator = " ")
collection_trigrams <- tokens_ngrams(collection_tokens, n = 3, concatenator = " ")
collection_quadrigrams <- tokens_ngrams(collection_tokens, n = 4, concatenator = " ")
collection_ngrams <- tokens_ngrams(collection_tokens, n = 1:4, concatenator = " ")

# Document Frequency Matrices

uni_dfm <- dfm(collection_unigrams)
bi_dfm <- dfm(collection_bigrams)
tri_dfm <- dfm(collection_trigrams)
quadri_dfm <- dfm(collection_quadrigrams)
corpus_dfm <- dfm(collection_sample, groups = "type")

```

## Modeling

```{r}

# Documetn Frequency Matrix




```

